name: speech2gesture

train_data_path: data/ted_dataset/lmdb_train
val_data_path: data/ted_dataset/lmdb_val
test_data_path: data/ted_dataset/lmdb_test

model_save_path: output/train_speech2gesture
random_seed: -1

# model params
model: speech2gesture
mean_dir_vec: [ 0.0154009, -0.9690125, -0.0884354, -0.0022264, -0.8655276, 0.4342174, -0.0035145, -0.8755367, -0.4121039, -0.9236511, 0.3061306, -0.0012415, -0.5155854,  0.8129665,  0.0871897, 0.2348464,  0.1846561,  0.8091402,  0.9271948,  0.2960011, -0.013189 ,  0.5233978,  0.8092403,  0.0725451, -0.2037076, 0.1924306,  0.8196916]
mean_pose: [ 0.0000306,  0.0004946,  0.0008437,  0.0033759, -0.2051629, -0.0143453,  0.0031566, -0.3054764,  0.0411491,  0.0029072, -0.4254303, -0.001311 , -0.1458413, -0.1505532, -0.0138192, -0.2835603,  0.0670333,  0.0107002, -0.2280813,  0.112117 , 0.2087789,  0.1523502, -0.1521499, -0.0161503,  0.291909 , 0.0644232,  0.0040145,  0.2452035,  0.1115339,  0.2051307]

# train params
epochs: 100
batch_size: 128
learning_rate: 0.001
loss_regression_weight: 100
loss_gan_weight: 10.0

# eval params
eval_net_path: output/train_h36m_gesture_autoencoder/gesture_autoencoder_checkpoint_best.bin

# dataset params
motion_resampling_framerate: 15
n_poses: 34
n_pre_poses: 4
subdivision_stride: 10
loader_workers: 3
